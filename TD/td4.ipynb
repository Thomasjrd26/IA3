{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TD4 IA** - Le perceptron, fonctions logiques et le problème du XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle du perceptron avec une fonction d'activation Heaviside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unit_step(v):\n",
    "    \"\"\" \n",
    "    Heavyside Step function. v must be a scalar \n",
    "    \"\"\"\n",
    "    if v >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def perceptron(x, w, b):\n",
    "    \"\"\" \n",
    "    Function implemented by a perceptron with weight vector w and bias b \n",
    "    y = f(w*x + b)\n",
    "    \"\"\"\n",
    "    v = np.dot(w, x) + b\n",
    "    y = unit_step(v)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NON logique (NOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w = 1, b = -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT_percep(x):\n",
    "    return perceptron(x, w=-1, b=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test du perceptron implémentant un OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT(0) = 1\n",
      "NOT(1) = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NOT(0) = {}\".format(NOT_percep(0)))\n",
    "print(\"NOT(1) = {}\".format(NOT_percep(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET logique (AND)\n",
    "w1 = 1, w2 = 1, b = -1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND(1, 1) = 1\n",
      "AND(1, 0) = 0\n",
      "AND(0, 1) = 0\n",
      "AND(0, 0) = 0\n"
     ]
    }
   ],
   "source": [
    "def AND_percep(x):\n",
    "    # weights: w1 = 1, w2 = 1\n",
    "    w = np.array([1, 1])\n",
    "    # biais\n",
    "    b = -1.5\n",
    "    return perceptron(x, w, b)\n",
    "\n",
    "# Test\n",
    "example1 = np.array([1, 1])\n",
    "example2 = np.array([1, 0])\n",
    "example3 = np.array([0, 1])\n",
    "example4 = np.array([0, 0])\n",
    "\n",
    "print(\"AND({}, {}) = {}\".format(1, 1, AND_percep(example1)))\n",
    "print(\"AND({}, {}) = {}\".format(1, 0, AND_percep(example2)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 1, AND_percep(example3)))\n",
    "print(\"AND({}, {}) = {}\".format(0, 0, AND_percep(example4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OU logique (OR)\n",
    "w1 = 1, w2 = 1, b = -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR(1, 1) = 1\n",
      "OR(1, 0) = 1\n",
      "OR(0, 1) = 1\n",
      "OR(0, 0) = 0\n"
     ]
    }
   ],
   "source": [
    "def OR_percep(x):\n",
    "    # weights: w1 = 1, w2 = 1\n",
    "    w = np.array([1, 1])\n",
    "    # biais\n",
    "    b = -0.5\n",
    "    return perceptron(x, w, b)\n",
    "\n",
    "# Test\n",
    "example1 = np.array([1, 1])\n",
    "example2 = np.array([1, 0])\n",
    "example3 = np.array([0, 1])\n",
    "example4 = np.array([0, 0])\n",
    "\n",
    "print(\"OR({}, {}) = {}\".format(1, 1, OR_percep(example1)))\n",
    "print(\"OR({}, {}) = {}\".format(1, 0, OR_percep(example2)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 1, OR_percep(example3)))\n",
    "print(\"OR({}, {}) = {}\".format(0, 0, OR_percep(example4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est-il **possible de trouver des paramètres pour un simple perceptron (w1, w2 et b)** de tel sorte qu'il **resolve le problème du OU Exclusif (XOR)** ?<br>\n",
    "Inutile de chercher, la réponse est non, **ces paramètres n'existent pas** !<br>\n",
    "La raison est que le **problème du XOR n'est pas linérairement séparable**.\n",
    "\n",
    "La solution consiste à **combiner de multiples séparateurs linéaires** en introduisant des **unités dites \"cachées\"** dans les réseaux : un **perceptron multi-couches** !<br>\n",
    "Mais le **nombre de paramètres augmente** et la **recherche des bonnes valeurs** de paramètres devient **compliquée**...<br>\n",
    "Or, vous savez à présent entrainer des réseaux pour leur faire apprendre des fonctions...<br>\n",
    "La **solution** consiste donc **entrainer un perceptron multi-couches** pour lui faire **apprendre la fonction XOR** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OU Exclusif logique (XOR) : une combinaison de fonctions logiques de base\n",
    "- Avant cela, on peut aussi vérifier que la fonction OU EXCLUSIF entre 2 entrées x1 et x2 peut se réaliser à partir des fonctions logiques de bases. \n",
    "- On peut en effet écrire : `XOR(x1, x2) = AND(NOT(AND(x1,x2)), OR(x1,x2))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR(1, 1) = 0\n",
      "XOR(1, 0) = 1\n",
      "XOR(0, 1) = 1\n",
      "XOR(0, 0) = 0\n"
     ]
    }
   ],
   "source": [
    "def XOR_net(x):\n",
    "    gate_1 = AND_percep(x)\n",
    "    gate_2 = NOT_percep(gate_1)\n",
    "    gate_3 = OR_percep(x)\n",
    "    new_x = np.array([gate_2, gate_3])\n",
    "    output = AND_percep(new_x)\n",
    "    return output\n",
    "\n",
    "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_net(example1)))\n",
    "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_net(example2)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_net(example3)))\n",
    "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_net(example4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage de la fonction logique XOR par un réseau de neurone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Données d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_train = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">**Exo1**</font> : Construction d'un perceptron multi-couches pour l'apprentissage de la fonction XOR\n",
    "- Construire un modèle de perceptron multi-couches avec 1 couche cachée de 8 neurones\n",
    "- Vous utiliserez une fonction d'activation `tanh` pour les neurones de la couche cachée et une fonction d'activation `sigmoid`pour les neurones de la couche de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# A COMPLETER..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 1\n",
    "nb_epoch = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilation du modèle\n",
    "- on utilise l'algorithme de Stochastic Gradient Descent (`SGD`) comme optimizer\n",
    "- il s'agit d'un problème de classification binaire, on utilise donc une loss de type `binary_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde les poids du modèle avant entrainement. On pourra ainsi les recharger plus loin pour recommencer un apprentissage de 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">**Exo2**</font> : Entrainement du modèle\n",
    "- Entrainer votre modèle pour 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(...) # A COMPLETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut afficher les poids du réseau après apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation : vérifions que le réseau a bien appris une fonction logique XOR !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(x_train))\n",
    "print(model.predict(x_train).round())   # On fait un arrondi pour avoir des valeurs entières 0 ou 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affichons enfin la courbe de la loss et de l'accuracy en fonction du nombre d'epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history of the loss and Accuracy\n",
    "plt.plot(history.history['loss'], label='Loss (training data)')\n",
    "plt.plot(history.history['accuracy'], label='Accuracy (training data)')\n",
    "plt.title('Loss for XOR')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">**Exo3**</font> : Early Stopping de l'apprentissage (arrêt prématuré)\n",
    "- La figure ci-dessus montre que la loss n'évolue plus beaucoup après environ 500 epoch\n",
    "- On pourrait donc pu arrêter l'entrainement avant la fin (i.e. les 1000 epochs)...\n",
    "- Keras a prévu ce type de situation et permet de définir une callback `EarlyStopping` dont la documentation se trouve [ici](https://keras.io/api/callbacks/early_stopping/).\n",
    "- Etudier les paramètres passés à la callback `EarlyStopping` ci-dessous. \n",
    "- Sous quelle condition l'apprentissage va-t-il s'arrêter prématurément ?\n",
    "- Réentrainer votre modèle avec le code ci-dessous et observer le nombre d'epoch lors de l'arrêt de l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avant de relancer un entrainement, on restaure les poids aléatoires initiaux (avant entrainement)\n",
    "model.load_weights('model.h5')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition d'une callback pour éventuellement arrêter l'entrainement avant le nombre d'epoch \n",
    "# Ici, on monitore la loss (i.e. l'erreur). Si la loss n'évolue pas de 0.0001 pendant 3 epochs, on s'arrete \n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=3, verbose=1, mode=\"min\", \n",
    "                                            baseline=None, restore_best_weights=True)\n",
    "    \n",
    "history = model.fit(x_train, y_train, epochs=nb_epoch, batch_size=batch_size, callbacks=[callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(x_train))\n",
    "print(model.predict(x_train).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history of the loss and Accuracy\n",
    "plt.plot(history.history['loss'], label='Loss (training data)')\n",
    "plt.plot(history.history['accuracy'], label='Accuracy (training data)')\n",
    "plt.title('Loss for XOR')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">**Exo4**</font> : \n",
    "* Construisez cette fois-ci un réseau de neurones avec 2 couches cachées de 8 neurones chacune\n",
    "* Vous utiliserez toujours une fonction d'activation `tanh` pour les neurones des couches cachées et une fonction d'activation `sigmoid`pour les neurones de la couche de sortie\n",
    "* Est-ce que le réseau apprend plus vite ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vos réponses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP avec 2 couches cachées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "# A COMPLETER..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(learning_rate=learning_rate)  # on utilise l'algorithme de Stochastic Gradient Descent (SGD)\n",
    "model2.compile(loss='binary_crossentropy',    # il s'agit d'un problème de classification binaire\n",
    "               optimizer=sgd, \n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_weights('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition d'une callback pour éventuellement arrêter l'entrainement avant le nombre d'epoch \n",
    "# Ici, on monitore la loss (i.e. l'erreur). Si la loss n'évolue pas de 0.0001 pendant 3 epochs, on s'arrete \n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=3, verbose=1, mode=\"min\", \n",
    "                                            baseline=None, restore_best_weights=True)\n",
    "    \n",
    "history2 = model2.fit(x_train, y_train, epochs=nb_epoch, batch_size=batch_size, callbacks=[callback], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2.predict(x_train))\n",
    "print(model2.predict(x_train).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history of the loss and Accuracy\n",
    "plt.plot(history2.history['loss'], label='Loss (training data)')\n",
    "plt.plot(history2.history['accuracy'], label='Accuracy (training data)')\n",
    "plt.title('Loss for XOR')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">**Exo5**</font> : Entrainer à nouveau votre modèle MLP après avoir changer les hyperparamètres\n",
    "* dimensions du réseau, \n",
    "* learning rate, \n",
    "* optimizer, \n",
    "* fonction d'activation... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
