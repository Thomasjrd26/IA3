{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TD2 IA - Classify hand-written digits using a Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objectives :\n",
    " - Recognizing handwritten digits\n",
    " - Understanding the principle of a classifier Dense Neural Network (DNN)\n",
    " - Implementation using Keras/TensorFlow \n",
    "\n",
    "\n",
    "The [MNIST dataset](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) (Modified National Institute of Standards and Technology) is a must for Deep Learning.  \n",
    "It consists in __60,000 small images__ of handwritten digits for __learning__ and __10,000__ for __testing__.\n",
    "\n",
    "\n",
    "## What we're going to do :\n",
    "\n",
    " - Retrieve data\n",
    " - Preparing the data\n",
    " - Create a model\n",
    " - Train the model\n",
    " - Evaluate the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Import and Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import sys,os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's print the tools version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensor Flow version : \" + tf.__version__)\n",
    "print(\"Keras version : \" + keras.__version__)\n",
    "print(\"Numpy version : \" + np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Retrieve data\n",
    "MNIST is one of the most famous historic dataset.  \n",
    "This dataset is available from [Keras datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset from Keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Get images and labels for train and test \n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Display the dimensions of the dataset\n",
    "print(\"train_images : \",train_images.shape)\n",
    "print(\"train_labels : \",train_labels.shape)\n",
    "print(\"test_images  : \",test_images.shape)\n",
    "print(\"test_labels  : \",test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Affichage des valeurs des pixels pour une image pour les données d'entrainement\n",
    "# Chaque pixel est codé par une valeur sur 8 bits non signée (uint8) \n",
    "digit = train_images[2]\n",
    "print(digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du nombre de labels pour le training\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des labels pour les données d'entrainement\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du nombre de labels pour le test\n",
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des labels pour les données de test\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">**Exo1**</font> : \n",
    " - Quel est le type des données train_images, train_labels, test_images et test_labels?\n",
    " - Quelle est la dimension des images d'entrée ?\n",
    " - Quel est le label du troisème digit d'entrainement ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Vos réponses :\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize one sample from the training set (using Matplotlib)\n",
    "plt.imshow(digit, cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the corresponding label...\n",
    "print(f\"Label = {train_labels[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Preparing the data\n",
    "\n",
    "#### Before training the model, data need to be preprocessed to\n",
    " - Get the data in the form expected by the network (`reshape`)\n",
    " - Get all input values between [0, 1] (**normalization**)\n",
    " \n",
    "#### Reshape and normalize the data\n",
    "  - Multi-Layer Perceptrons do not understand '2D' inputs...\n",
    "  - So, we need to reshape 28x28 pixels input images (a matrix) to a **784 features input vector**\n",
    "  - To do so, we transform an array in the form (60000, 28, 28) with values of type uint8 between 0 and 255, \n",
    "    to **an array in the form (60000, 28*28) of type float32** with values **between 0 and 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape\n",
    "feature_vector_length = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# Comme les images sont les entrées, on les nomme x\n",
    "print(f\"train_images shape = {train_images.shape}\")\n",
    "x_train = train_images.reshape(len(train_images), feature_vector_length)\n",
    "print(f\"x_train shape = {x_train.shape}\")\n",
    "x_test = test_images.reshape(len(test_images), feature_vector_length)\n",
    "\n",
    "\n",
    "# Normalize\n",
    "print('Before normalization : Min={}, max={}'.format(x_train.min(),x_train.max()))\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "print('After normalization  : Min={}, max={}'.format(x_train.min(),x_train.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the labels\n",
    "\n",
    "Labels needs to be coded as categorical. As there will be 10 output classes, the label `3` for instance will be coded as `0001000000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target classes to categorical ones\n",
    "print('Before categorical coding : {}'.format(train_labels[3]))\n",
    "print('Before categorical coding : {}'.format(train_labels[4]))\n",
    "\n",
    "# Comme les labels sont les sorties, on les nomme y\n",
    "y_train = to_categorical(train_labels, num_classes)\n",
    "y_test = to_categorical(test_labels, num_classes)\n",
    "\n",
    "print('After categorical coding : {}'.format(y_train[3]))\n",
    "print('After categorical coding : {}'.format(y_train[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Build a model with Keras\n",
    "\n",
    "Let's build a first model of neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(28*28,), activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# Softmax, the activation function, is capable of generating a so-called multiclass probability distribution. \n",
    "# That is, it computes the probability that a certain feature vector belongs to one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">**Exo2**</font> : \n",
    "* Quel est le rôle de la fonction `softmax` utilisée pour la couche de sortie ? Que retourne cette fonction?\n",
    "* Combien de couches comportent le modèle? Combien y a-t-il de couche(s) cachée(s) ?\n",
    "* Indiquer le nombre de neurones par couches\n",
    "* Retrouver par le calcul le nombre de paramètres entrainables du modèle.\n",
    " \n",
    "<u>Remarque</u> : plus le nombre de paramètres entrainables est grand, plus le modèle sera long à entrainer et plus il nécessitera de la mémoire pour stocker ces paramètres (empreinte mémoire) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Vos réponses</u>: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's initialize hyper parameters before traingin the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 32           # an epoch is one complete pass through the training data (all the training data are seen by the model)\n",
    "                      # here, the model will see 20 times the complete training set \n",
    "batch_size = 64       # the batch size is the number of training samples processed before the model is updated, \n",
    "                      # in other words the number of training examples in one forward/backward pass. \n",
    "                      # The higher the batch size, the more memory space you will need but the faster the training time.\n",
    "# The number of iterations per epoch is defined as the ratio between the training set and the batch size.\n",
    "# Here, it wiil be equal to (60000*0.8)/64 = 750\n",
    "learning_rate = 0.01  # the learning rate controls how much to change the model in response to the estimated error \n",
    "                      # each time the model weights are updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compile the model. \n",
    "When compiling the model, we need to indicate :\n",
    " - an **optimizer** : the purpose of an optimizer is to adjust model parameters (weights and biaises) to minimize an error function (i.e. the loss)\n",
    " - a **loss function** : it is a function that compares the target and predicted output values; it measures how well the neural network models the training data. When training, we aim to minimize this loss between the predicted and target outputs.\n",
    " - some **metrics** : metrics are used to monitor and measure the performance of a model (during training and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Let's train the model\n",
    " - Keras fit is the method used for the model training on the data set for the specified number of fixed epochs.\n",
    " - Here, 20% of the training dataset is used for validation, i.e. to evaluate the model during the training\n",
    " - So, only 80% of the dataset is used for training.\n",
    " - The number of iterations per epoch is therefore equal to : (60000*0.8)/batch_size = 750 iterations\n",
    " - At each iteration, the parameters (weights and biaises) are updated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation split of 20%: splits the 60.000 training samples into 48.000 for training and 12.000 for validation\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs = epochs, \n",
    "                    batch_size = batch_size, \n",
    "                    verbose = 1, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 6 - Evaluate**\n",
    "\n",
    "Let's evaluate the model after training on **10000 test images**.<br>\n",
    "This part of the dataset (the test) has **never been seen before by the model**.<br>\n",
    "It is therefore a way to evaluate the **capability of the model to generalize** on new data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The batch size is by default equal to 32 for the Keras evaluate function\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f'Test results - Loss: {score[0]} - Accuracy: {score[1]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's display the history of the loss according to the number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history of the loss\n",
    "plt.plot(history.history['loss'], label='Loss (training data)')\n",
    "plt.plot(history.history['val_loss'], label='loss (validation data)')\n",
    "plt.title('Loss for MNIST')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's display the history of the accuracy according to the number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "plt.plot(history.history['accuracy'], label='Acc (training data)')\n",
    "plt.plot(history.history['val_accuracy'], label='Acc (validation data)')\n",
    "plt.title('Accuracy for MNIST')\n",
    "plt.ylabel('MSE value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <font color=\"red\">**Exo3**</font> : \n",
    " - Quelle est en pourcentage l'accuracy du modèle sur les 10000 images de test ? \n",
    " - Commenter les courbes\n",
    "\n",
    "<u>Attention</u>: il faut se méfier de l'accuracy que nous avons obtenue. En effet, celle-ci a été calculée à partir d'un seul entrainement du modèle... Pour évaluer correctement le modèle (d'un point de vue statistique), il faudrait entrainer et évaluer le modèle plusieurs fois (au moins 5 fois) et calculer alors l'accuracy moyenne sur l'ensemble de ces \"run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vos réponses : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <font color=\"red\">**Exo4**</font> : un second MLP\n",
    " - Construire un nouveau modèle (model2) comportant cette fois-ci 2 couches cachées de 100 neurones chacune\n",
    " - Retrouver par le calcul le nombre de paramètres entrainables du modèle\n",
    " - Compiler, entrainer et évaluer ce modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model (A COMPLETER)\n",
    "model2 = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Votre réponse</u>: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model2.compile(optimizer = optimizer2, \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(x_train, y_train, \n",
    "                      epochs = epochs, \n",
    "                      batch_size = batch_size, \n",
    "                      verbose = 1, \n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the batch size is by default equal to 32 for the Keras evaluate function\n",
    "score = model2.evaluate(x_test, y_test, verbose=1)\n",
    "print(f'Test results - Loss: {score[0]} - Accuracy: {score[1]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's display the history of the loss according to the number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "plt.plot(history2.history['loss'], label='Loss (training data)')\n",
    "plt.plot(history2.history['val_loss'], label='loss (validation data)')\n",
    "plt.title('Loss for MNIST')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's display the history of the accuracy according to the number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "plt.plot(history2.history['accuracy'], label='Acc (training data)')\n",
    "plt.plot(history2.history['val_accuracy'], label='Acc (validation data)')\n",
    "plt.title('Accuracy for MNIST')\n",
    "plt.ylabel('MSE value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">**Exo5**</font> : \n",
    "* Quelle est en pourcentage l'accuracy de ce 2nd modèle sur les 10000 images de test ? \n",
    "* Commenter les courbes\n",
    "* Conclure (comparaison des 2 modèles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vos réponses : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a trained model\n",
    "\n",
    " - L'entrainement d'un modèle de réseaux de neurones peut être très long (plusieurs heures, voire plusieurs jours !!)\n",
    " - Il est donc souvent judicieux de sauvegarder le modèle entrainé.\n",
    " - Il existe différents formats pour cela\n",
    " - **Keras** possède un **format de sauvegarde** utilisant le HDF5 dont l'extension est .h5 (https://en.wikipedia.org/wiki/Hierarchical_Data_Format).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voici la commande pour **sauvegarder le modèle entrainé** sur votre disque (en fait sur le disque de la machine virtuelle de Colab) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici, nous sauvegardons le 1er modèle avce 1 seule couche cachée\n",
    "model2.save('MLP_MNIST_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On peut ainsi **recharger le modèle** depuis le fichier sauvegardé:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recréé exactement le même model, incluant poids et optimizer.\n",
    "loaded_model = keras.models.load_model('MLP_MNIST_model.h5')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Puis **évaluer de nouveau le modèle** rechargé:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = loaded_model.evaluate(x_test, y_test, verbose=1)\n",
    "#print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')\n",
    "print(f'Test results - Loss: {loss} - Accuracy: {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
